{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# cuDNN 관련 설정\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통합 파이프라인 함수\n",
    "def full_training_pipeline(train_df, output_dir, base_model_path, adapter_model_path, epoch):\n",
    "    # 8-bit BitsAndBytes config (8-bit uses bfloat16)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True\n",
    "    )\n",
    "\n",
    "    model_id = 'beomi/gemma-ko-7b'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        cache_dir=\"/workspace/huggingface\"    # <--- volume 지정\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=\"/workspace/huggingface\"    # <--- volume 지정\n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    train = Dataset.from_pandas(train_df)\n",
    "\n",
    "    def prompting(input, output):\n",
    "        prompt = (\n",
    "            \"<start_of_turn> Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. The number of words and letters per word must be observed.\\n\"\n",
    "            f\"Input: {input}\\n\"\n",
    "            \"<end_of_turn>\\n\"\n",
    "            \"<start_of_turn>Assistant:\\n\"\n",
    "            f\"Output: {output}\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def chat_format(row):\n",
    "        prompt = prompting(row[\"input\"], row[\"output\"])\n",
    "        tokens = tokenizer.encode(prompt, truncation=True, max_length=512)\n",
    "        row[\"input_ids\"] = tokens\n",
    "        return row\n",
    "\n",
    "    train = train.map(chat_format, batched=False, num_proc=4)\n",
    "\n",
    "    # LoRA 설정\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"down_proj\", \"up_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM'\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.train()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        seed=42,\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epoch,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        eval_strategy=\"no\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=50,\n",
    "        warmup_steps=20,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=2e-4,\n",
    "        group_by_length=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=lambda x: x['input_ids']\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 저장 및 로드\n",
    "    trainer.model.save_pretrained(adapter_model_path)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"/workspace/huggingface\")\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        adapter_model_path,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #2차 해독_finetuning_ver1(input:MLP)\n",
    "    train0 = pd.read_csv('/workspace/data/finetuning_train_mlp.csv')\n",
    "    output_dir0=\"/workspace/results0221\"\n",
    "    base_model_path0 = \"beomi/gemma-ko-7b\"\n",
    "    adapter_model_path0 = \"/workspace/results0221/lora-adapter-epoch3\"\n",
    "\n",
    "    model0 = full_training_pipeline(train0, output_dir0, base_model_path0, adapter_model_path0, epoch=3)\n",
    "    model0.save_pretrained(\"/workspace/results0221/gemma-finetuning-epoch3\")\n",
    "    print(\"2차 해독_finetuning_ver1(input:MLP) 완료\")\n",
    "\n",
    "    #2차 해독_finetuning_ver1(input:원본)\n",
    "    train1 = pd.read_csv('/workspace/data/finetuning_train_original.csv')\n",
    "    output_dir1=\"/workspace/results0225\"\n",
    "    base_model_path1 = \"beomi/gemma-ko-7b\"\n",
    "    adapter_model_path1 = \"/workspace/results0225/lora-adapter-epoch5\"\n",
    "\n",
    "    model1 = full_training_pipeline(train1, output_dir1, base_model_path1, adapter_model_path1, epoch=5)\n",
    "    model1.save_pretrained(\"/workspace/results0225/gemma-finetuning-epoch5\")\n",
    "    print(\"2차 해독_finetuning_ver1(input:원본) 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_reviews(FINETUNE_MODEL, BASE_MODEL, test):\n",
    "    finetune_model = AutoModelForCausalLM.from_pretrained(\n",
    "        FINETUNE_MODEL, cache_dir=\"/workspace/huggingface\", device_map={\"\": 0}\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=\"/workspace/huggingface\")\n",
    "\n",
    "    text_gen_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=finetune_model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    restored_reviews = []\n",
    "    total_reviews = len(test)\n",
    "\n",
    "    with tqdm(total=total_reviews, desc=\"Processing\", unit=\"review\") as pbar:\n",
    "        for index, row in test.iterrows():\n",
    "            query = row['input']\n",
    "            prompt = (\n",
    "                \"<start_of_turn> Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. The number of words and letters per word must be observed.\\n\"\n",
    "                f\"Input: {query}\\n\"\n",
    "                \"<end_of_turn>\\n\"\n",
    "                \"<start_of_turn>Assistant:\\n\"\n",
    "                \"Output:\"\n",
    "            )\n",
    "\n",
    "            generated = text_gen_pipeline(\n",
    "                prompt,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.2,\n",
    "                top_p=0.9,\n",
    "                max_new_tokens=len(query),\n",
    "                do_sample=True,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            generated_text = generated[0]['generated_text']\n",
    "            output_start = generated_text.find(\"Output:\")\n",
    "\n",
    "            if output_start != -1:\n",
    "                restored_reviews.append(generated_text[output_start + len(\"Output:\"):].strip())\n",
    "            else:\n",
    "                restored_reviews.append(generated_text.strip())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    return restored_reviews\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  FINETUNE_MODEL0 = \"/workspace/results0221/gemma-finetuning-epoch3\"\n",
    "  BASE_MODEL0 = \"beomi/gemma-ko-7b\"\n",
    "  test0 = pd.read_csv('/workspace/finetuning_test_mlp.csv')\n",
    "  restored_reviews0 = restore_reviews(FINETUNE_MODEL0, BASE_MODEL0, test0)\n",
    "\n",
    "  FINETUNE_MODEL1 = \"/workspace/results0225/gemma-finetuning-epoch5\"\n",
    "  BASE_MODEL1 = \"beomi/gemma-ko-7b\"\n",
    "  test1 = pd.read_csv('/workspace/finetuning_test_original.csv')\n",
    "  restored_reviews1 = restore_reviews(FINETUNE_MODEL1, BASE_MODEL1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/workspace/data/sample_submission.csv\")\n",
    "submission = pd.DataFrame()\n",
    "submission['output'] = restored_reviews0\n",
    "submission['output'] = submission['output'].apply(lambda x: x.split(\"<end_of_turn>\")[0])\n",
    "submission.to_csv('/workspace/kogemma_0222_raw.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/workspace/data/sample_submission.csv\")\n",
    "submission = pd.DataFrame()\n",
    "submission['output'] = restored_reviews1\n",
    "submission['output'] = submission['output'].apply(lambda x: x.split(\"<end_of_turn>\")[0])\n",
    "submission.to_csv('/workspace/kogemma_0226_raw.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
